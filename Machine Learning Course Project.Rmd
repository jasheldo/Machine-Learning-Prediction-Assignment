---
title: "Machine Learning Course Project"
author: "James Sheldon"
date: "June 15, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lattice)
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
```

## Executive Summary
Exercising is an important part of a healthy lifestyle. However, done incorrectly, exercising can lead to negative outcomes such as injury or even death. Six participants were part of a study where they exercised while having accelerometers attached to various parts of their body. These accelerometers recorded motion data for dumbell lifts and these exercises were graded on a scale of A to E based on the quality of the motion.

In this report we take the freely available motion data and apply a learning algorithm to them in order to predict the quality of a set of a separate test data.

## Data Preprocessing

As part of the exploratory analysis (bottom of this report), we identified a number of variables that are obviously not useful for the predictive process.  Those include timestamps, participant ID, and other factors that have a near zero variance.  These variables were removed from the dataset used for model building purposes without loss of generality.

```{r data}
set.seed(35354)
setwd("~/GitHub/Machine-Learning-Prediction-Assignment")
traindata <- read.csv("pml-training.csv", header = TRUE, sep = ",", stringsAsFactors = TRUE, na.strings = "NA")
testdata <- read.csv("pml-testing.csv", header = TRUE, sep = ",", stringsAsFactors = TRUE, na.strings = "NA")

traindata <- tbl_df(traindata)
testdata <- tbl_df(testdata)

# Remove variables that have a near zero variance as determined in the exploratory analysis. Further remove variables that clearly are not impactful as a predictor

traindatasub <- select(traindata, -(kurtosis_roll_belt:var_yaw_belt),-(X:num_window),-(var_accel_arm:var_yaw_arm), -(kurtosis_roll_arm:amplitude_yaw_arm), -(kurtosis_roll_dumbbell:amplitude_yaw_dumbbell), -(var_accel_dumbbell:var_yaw_dumbbell), -(kurtosis_roll_forearm:amplitude_yaw_forearm), -(var_accel_forearm:var_yaw_forearm))

testdatasub <- select(testdata, -(kurtosis_roll_belt:var_yaw_belt),-(X:num_window),-(var_accel_arm:var_yaw_arm), -(kurtosis_roll_arm:amplitude_yaw_arm), -(kurtosis_roll_dumbbell:amplitude_yaw_dumbbell), -(var_accel_dumbbell:var_yaw_dumbbell), -(kurtosis_roll_forearm:amplitude_yaw_forearm), -(var_accel_forearm:var_yaw_forearm))
```

## Model Build

We now take the cleaned test data set and generate two subsets: one is the training set and the other is the testing set. We then apply the randomForest R package to the training set to generate a model, review the results and if sufficient, cross validate the model against the testing data.

The randomForest modeling method was used becuase it seemed the most appropriate given the number of variables remaining in the data for fitting. 

```{r model build}
inTrain <- createDataPartition(y=traindatasub$classe, p=0.6,list=FALSE)
myTrain <- traindatasub[inTrain,]
myTest <- traindatasub[-inTrain,]

modFit <- randomForest(classe ~ .,data = myTrain)
print(modFit)
```

As you can see from the above summary, this model has a maximum class error of 1.34% with an out of sample estimated error of 0.6%. This is a pretty solid model suggesting the randomForest was the correct model choice.

Let's cross validate against the sample testing data to see how the model stacks up.

```{r cross validate}
# Cross Validate
pmod <- predict(modFit, myTest, type = "class")
confusionMatrix(myTest$classe,pmod)

#In Sample Error
ptrain <- predict(modFit, myTrain, type = "class")
confusionMatrix(myTrain$classe,ptrain)
```

Very nice! We have an accuracy rate of 99.54%. We will use this model to perform our predictions on the test data.

## Model Prediction

```{r test prediction}
test_prediction <- predict(modFit, testdatasub, type = "class")
print(test_prediction)
```

## Exploratory Analysis

```{r exploring}
glimpse(traindata)

# Identify variables with near zero variance = TRUE.  This forms the basis for the above variable removal.
nearZeroVar(traindata, saveMetrics = TRUE)

# Some additional (not necessary) exploratory analysis for information purposes.
traindatasub %>% group_by(classe) %>% summarise(avg.forearm = mean(total_accel_forearm), avg.dumbbell = mean(total_accel_dumbbell), avg.arm = mean(total_accel_arm), avg.belt = mean(total_accel_belt)) %>% arrange(classe)
```